# DE_course_final_task

Задание:

Есть csv файл, расположенный по ссылке: https://disk.yandex.ru/d/OdhP0RmgPWSAVw
Количество строк - 590708 строк.

Данные в файле:
ID объекта (например, 590707)
Координаты (Широта, Долгота) (например, 55.060199, 32.695577)
Год постройки (например, 1953.0)
Площадь (например, 585.60)
Количество этажей (например, 18)
Область и город (например, Смоленская область, Ярцево)
Адрес (например, "ул. Братьев Шаршановых, д. 61")
Описание объекта (например, "Жилой дом в Ярцево, по адресу ул. Братьев Шаршановых, д. 61, 1953 года постройки, под управлением ТСЖ «Шаршановых».")


В проекте используются Airflow, PySpark и Clickhouse.  

0. Предварительно развернуть docker-compose.   Добавить туда поддержку pyspark.  (в выложенном тут docker-compose она уже добавлена)

	Шаги 1-7 должны быть сделаны в DAG.

1. Загрузите файл данных в DataFrame PySpark. Обязательно выведите количество строк.

2. Убедиться, что данные корректно прочитаны (правильный формат, отсутствие пустых строк).

3. Преобразовать текстовые и числовые поля в соответствующие типы данных (например, дата, число).

4. Вычислить средний и медианный год постройки зданий.

5. Определить топ-10 областей и городов с наибольшим количеством объектов.

6. Найти здания с максимальной и минимальной площадью в рамках каждой области.

7. Определить количество зданий по десятилетиям (например, сколько зданий построено в 1950-х, 1960-х и т.д.).

8. Создайть схему таблицы в ClickHouse, которая будет соответствовать структуре  данных. 

9. Настроить соединение с ClickHouse из скрипта ( необходимо сделать в airflow).

10. Загрузить обработанные данные из DataFrame в таблицу в ClickHouse (airflow)

11. Выполнить SQL скрипт в Python, который выведет топ 25 домов, у которых площадь больше 60 кв.м (airflow)

Дополнительно. Cделать графики на MatPlotLib для 5, 6 и 7 пунктов.


Файлы в репозитории:
docker-compose.yml  - отредактированный docker-compose. В нем прописана ссылка на файл Dockerfile.airflow с  кастомным образом
                     airflow с добавлением туда openjdk и ряда библиотек airflow и python, необходимых для выполнения задания и работоспособности pyspark в среде airflow.
Dockerfile.airflow - см. выше.
main_spark.py  - единый DAG - файл, в котором выполняются все пункты задания, начиная с п.1. Загружаются данные из CSV файла. 
				(Путь по-умолчанию для csv-файла, прописанный в DAG: /opt/airflow/df/)
		        Очищаются, преобразуются, анализируются. Результаты анализа выводятся в логах Python. Строятся графики с помощью библиотек seaborn и matplotlib.
		        Графики выгружаются в папку: /opt/airflow/df/graphics.
		        Создается БД houses_db в Clickhouse, в ней создается таблица table_sp, в которую загружаются данные из датафрейма pyspark после их анализа.
		        Запускается SQL скрипт который выводит топ 25 домов, у которых площадь больше 60 кв.м. Результат выполнения скрипта выводится в логах Python соответствующего задания.

bonus/main_pandas.py - единый DAG - файл, в котором выполняются все пункты задания, начиная с п.1, но с использованием pandas вместо pyspark. 
				Пути для файлов теже самые.
				 Из отличий: создается своя отдельная таблица table_pd в базе houses_db, графики seaborn/matplotlib сохраняются также в папку /opt/airflow/df/graphics, 
				 ноимеют названия отличные от названий графиков, сделанных с помощью pyspark.
